{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jetson: \n",
      "GST_ARGUS: Creating output stream\n",
      "CONSUMER: Waiting until producer is connected...\n",
      "GST_ARGUS: Available Sensor modes :\n",
      "GST_ARGUS: 3280 x 2464 FR = 21.000000 fps Duration = 47619048 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 3280 x 1848 FR = 28.000001 fps Duration = 35714284 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1920 x 1080 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1640 x 1232 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1280 x 720 FR = 59.999999 fps Duration = 16666667 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: Running with following settings:\n",
      "   Camera index = 0 \n",
      "   Camera mode  = 4 \n",
      "   Output Stream W = 1280 H = 720 \n",
      "   seconds to Run    = 0 \n",
      "   Frame Rate = 59.999999 \n",
      "GST_ARGUS: Setup Complete, Starting captures for 0 seconds\n",
      "GST_ARGUS: Starting repeat capture requests.\n",
      "CONSUMER: Producer has connected; continuing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0] global /home/ubuntu/build_opencv/opencv/modules/videoio/src/cap_gstreamer.cpp (1100) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1\n"
     ]
    }
   ],
   "source": [
    "# Full reset of the camera\n",
    "!echo 'jetson' | sudo -S systemctl restart nvargus-daemon && printf '\\n'\n",
    "\n",
    "from jetcam.csi_camera import CSICamera\n",
    "\n",
    "camera = CSICamera(width=224, height=224)\n",
    "\n",
    "camera.running = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from xy_dataset import XYDataset\n",
    "\n",
    "TASK = 'road_following'\n",
    "\n",
    "CATEGORIES = ['apex']\n",
    "\n",
    "DATASETS = ['A', 'B']\n",
    "\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "datasets = {}\n",
    "for name in DATASETS:\n",
    "    datasets[name] = XYDataset(TASK + '_' + name, CATEGORIES, TRANSFORMS, random_hflip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a492ea930667498fb8359ed07062b483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(ClickableImageWidget(value='<video class=\"stream_widget_2775ed83b83c42268f496013…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "from clickable_stream_widget.widget import ClickableImageWidget\n",
    "\n",
    "\n",
    "# initialize active dataset\n",
    "dataset = datasets[DATASETS[0]]\n",
    "\n",
    "# unobserve all callbacks from camera in case we are running this cell for second time\n",
    "camera.unobserve_all()\n",
    "\n",
    "# create image preview\n",
    "camera_widget = ClickableImageWidget(width=camera.width, height=camera.height, encoder = \"vp8enc target-bitrate=1000000\")\n",
    "snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height)\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'data'))\n",
    "\n",
    "# create widgets\n",
    "dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')\n",
    "category_widget = ipywidgets.Dropdown(options=dataset.categories, description='category')\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "\n",
    "# manually update counts at initialization\n",
    "count_widget.value = dataset.get_count(category_widget.value)\n",
    "\n",
    "# sets the active dataset\n",
    "def set_dataset(change):\n",
    "    global dataset\n",
    "    dataset = datasets[change['new']]\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "dataset_widget.observe(set_dataset, names='value')\n",
    "\n",
    "# update counts when we select a new category\n",
    "def update_counts(change):\n",
    "    count_widget.value = dataset.get_count(change['new'])\n",
    "category_widget.observe(update_counts, names='value')\n",
    "\n",
    "\n",
    "def save_snapshot(_, content, msg):\n",
    "    if content['event'] == 'click':\n",
    "        data = content['eventData']\n",
    "        x = data['offsetX']\n",
    "        y = data['offsetY']\n",
    "        \n",
    "        # save to disk\n",
    "        dataset.save_entry(category_widget.value, camera.value, x, y)\n",
    "        \n",
    "        # display saved snapshot\n",
    "        snapshot = camera.value.copy()\n",
    "        snapshot = cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3)\n",
    "        snapshot_widget.value = bgr8_to_jpeg(snapshot)\n",
    "        count_widget.value = dataset.get_count(category_widget.value)\n",
    "        \n",
    "camera_widget.on_msg(save_snapshot)\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget, snapshot_widget]),\n",
    "    dataset_widget,\n",
    "    category_widget,\n",
    "    count_widget\n",
    "])\n",
    "\n",
    "display(data_collection_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jetson/.local/lib/python3.8/site-packages/torchvision-0.16.1-py3.8-linux-aarch64.egg/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jetson/.local/lib/python3.8/site-packages/torchvision-0.16.1-py3.8-linux-aarch64.egg/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed78d9487774e3b828acc82bb8e1b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='road_following_model.pth', description='model path'), HBox(children=(Button(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda')\n",
    "output_dim = 2 * len(dataset.categories)  # x, y coordinate for each category\n",
    "\n",
    "# ALEXNET\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "# SQUEEZENET \n",
    "# model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "# model.classifier[1] = torch.nn.Conv2d(512, output_dim, kernel_size=1)\n",
    "# model.num_classes = len(dataset.categories)\n",
    "\n",
    "# RESNET 18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# RESNET 34\n",
    "# model = torchvision.models.resnet34(pretrained=True)\n",
    "# model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# DENSENET 121\n",
    "# model = torchvision.models.densenet121(pretrained=True)\n",
    "# model.classifier = torch.nn.Linear(model.num_features, output_dim)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model_save_button = ipywidgets.Button(description='save model')\n",
    "model_load_button = ipywidgets.Button(description='load model')\n",
    "model_path_widget = ipywidgets.Text(description='model path', value='road_following_model.pth')\n",
    "\n",
    "def load_model(c):\n",
    "    model.load_state_dict(torch.load(model_path_widget.value))\n",
    "model_load_button.on_click(load_model)\n",
    "    \n",
    "def save_model(c):\n",
    "    torch.save(model.state_dict(), model_path_widget.value)\n",
    "model_save_button.on_click(save_model)\n",
    "\n",
    "model_widget = ipywidgets.VBox([\n",
    "    model_path_widget,\n",
    "    ipywidgets.HBox([model_load_button, model_save_button])\n",
    "])\n",
    "\n",
    "\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9b5bedf8d347edb2603b6954a1cad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'', format='jpeg', height='224', width='224'), ToggleButtons(description='state', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "from utils import preprocess\n",
    "import torch.nn.functional as F\n",
    "\n",
    "state_widget = ipywidgets.ToggleButtons(options=['stop', 'live'], description='state', value='stop')\n",
    "prediction_widget = ipywidgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "\n",
    "def live(state_widget, model, camera, prediction_widget):\n",
    "    global dataset\n",
    "    while state_widget.value == 'live':\n",
    "        image = camera.value\n",
    "        preprocessed = preprocess(image)\n",
    "        output = model(preprocessed).detach().cpu().numpy().flatten()\n",
    "        category_index = dataset.categories.index(category_widget.value)\n",
    "        x = output[2 * category_index]\n",
    "        y = output[2 * category_index + 1]\n",
    "        \n",
    "        x = int(camera.width * (x / 2.0 + 0.5))\n",
    "        y = int(camera.height * (y / 2.0 + 0.5))\n",
    "        \n",
    "        prediction = image.copy()\n",
    "        prediction = cv2.circle(prediction, (x, y), 8, (255, 0, 0), 3)\n",
    "        prediction_widget.value = bgr8_to_jpeg(prediction)\n",
    "            \n",
    "def start_live(change):\n",
    "    if change['new'] == 'live':\n",
    "        execute_thread = threading.Thread(target=live, args=(state_widget, model, camera, prediction_widget))\n",
    "        execute_thread.start()\n",
    "\n",
    "state_widget.observe(start_live, names='value')\n",
    "\n",
    "live_execution_widget = ipywidgets.VBox([\n",
    "    prediction_widget,\n",
    "    state_widget\n",
    "])\n",
    "\n",
    "display(live_execution_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93a1d0b8d024852a496f5638f3634c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntText(value=1, description='epochs'), FloatProgress(value=0.0, description='progress', max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "epochs_widget = ipywidgets.IntText(description='epochs', value=1)\n",
    "eval_button = ipywidgets.Button(description='evaluate')\n",
    "train_button = ipywidgets.Button(description='train')\n",
    "loss_widget = ipywidgets.FloatText(description='loss')\n",
    "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
    "\n",
    "def train_eval(is_training):\n",
    "    global BATCH_SIZE, LEARNING_RATE, MOMENTUM, model, dataset, optimizer, eval_button, train_button, accuracy_widget, loss_widget, progress_widget, state_widget\n",
    "    \n",
    "    try:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        state_widget.value = 'stop'\n",
    "        train_button.disabled = True\n",
    "        eval_button.disabled = True\n",
    "        time.sleep(1)\n",
    "\n",
    "        if is_training:\n",
    "            model = model.train()\n",
    "        else:\n",
    "            model = model.eval()\n",
    "\n",
    "        while epochs_widget.value > 0:\n",
    "            i = 0\n",
    "            sum_loss = 0.0\n",
    "            error_count = 0.0\n",
    "            for images, category_idx, xy in iter(train_loader):\n",
    "                # send data to device\n",
    "                images = images.to(device)\n",
    "                xy = xy.to(device)\n",
    "\n",
    "                if is_training:\n",
    "                    # zero gradients of parameters\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # execute model to get outputs\n",
    "                outputs = model(images)\n",
    "\n",
    "                # compute MSE loss over x, y coordinates for associated categories\n",
    "                loss = 0.0\n",
    "                for batch_idx, cat_idx in enumerate(list(category_idx.flatten())):\n",
    "                    loss += torch.mean((outputs[batch_idx][2 * cat_idx:2 * cat_idx+2] - xy[batch_idx])**2)\n",
    "                loss /= len(category_idx)\n",
    "\n",
    "                if is_training:\n",
    "                    # run backpropogation to accumulate gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # step optimizer to adjust parameters\n",
    "                    optimizer.step()\n",
    "\n",
    "                # increment progress\n",
    "                count = len(category_idx.flatten())\n",
    "                i += count\n",
    "                sum_loss += float(loss)\n",
    "                progress_widget.value = i / len(dataset)\n",
    "                loss_widget.value = sum_loss / i\n",
    "                \n",
    "            if is_training:\n",
    "                epochs_widget.value = epochs_widget.value - 1\n",
    "            else:\n",
    "                break\n",
    "    except e:\n",
    "        pass\n",
    "    model = model.eval()\n",
    "\n",
    "    train_button.disabled = False\n",
    "    eval_button.disabled = False\n",
    "    state_widget.value = 'live'\n",
    "    \n",
    "train_button.on_click(lambda c: train_eval(is_training=True))\n",
    "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
    "    \n",
    "train_eval_widget = ipywidgets.VBox([\n",
    "    epochs_widget,\n",
    "    progress_widget,\n",
    "    loss_widget,\n",
    "    ipywidgets.HBox([train_button, eval_button])\n",
    "])\n",
    "\n",
    "display(train_eval_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following widget can be used to label a multi-class x, y dataset.  It supports labeling only one instance of each class per image (ie: only one dog), but multiple classes (ie: dog, cat, horse) per image are possible.\n",
    "\n",
    "Click the image on the top left to save an image of ``category`` to ``dataset`` at the clicked location.\n",
    "\n",
    "| Widget | Description |\n",
    "|--------|-------------|\n",
    "| dataset | Selects the active dataset |\n",
    "| category | Selects the active category |\n",
    "| epochs | Sets the number of epochs to train for |\n",
    "| train | Trains on the active dataset for the number of epochs specified |\n",
    "| evaluate | Evaluates the accuracy on the active dataset over one epoch |\n",
    "| model path | Sets the active model path |\n",
    "| load | Loads a model from the active model path |\n",
    "| save | Saves a model to the active model path |\n",
    "| stop | Disables the live demo |\n",
    "| live | Enables the live demo |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba15c4eef994f5d9a44276a2afe6111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(HBox(children=(ClickableImageWidget(value='<video class=\"stream_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([data_collection_widget, live_execution_widget]), \n",
    "    train_eval_widget,\n",
    "    model_widget\n",
    "])\n",
    "\n",
    "display(all_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '_ipython_display_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/formatters.py:922\u001b[0m, in \u001b[0;36mIPythonDisplayFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    920\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/clickable_stream_widget-1.0.0-py3.8.egg/clickable_stream_widget/widget.py:318\u001b[0m, in \u001b[0;36mClickableImageWidget._ipython_display_\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ipython_display_\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ipython_display_\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '_ipython_display_'"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "new Promise(function(resolve, reject) {\n",
       "\tvar script = document.createElement(\"script\");\n",
       "\tscript.onload = resolve;\n",
       "\tscript.onerror = reject;\n",
       "\tscript.src = \"https://webrtc.github.io/adapter/adapter-latest.js\";\n",
       "\tdocument.head.appendChild(script);\n",
       "}).then(() => {\n",
       "\n",
       "                if (typeof html5VideoElement2775ed83b83c42268f496013e8a30f62 === \"undefined\") {\n",
       "                    var html5VideoElement2775ed83b83c42268f496013e8a30f62 = null;\n",
       "                    var websocketConnection2775ed83b83c42268f496013e8a30f62 = null;\n",
       "                    var webrtcPeerConnection2775ed83b83c42268f496013e8a30f62 = null;\n",
       "                    var webrtcConfiguration2775ed83b83c42268f496013e8a30f62 = null;\n",
       "                    var reportError2775ed83b83c42268f496013e8a30f62 = null;\n",
       "                    var remoteStream2775ed83b83c42268f496013e8a30f62 = null;\n",
       "                }\n",
       "\n",
       "                function onLocalDescription2775ed83b83c42268f496013e8a30f62(desc) {\n",
       "                    console.log(\"Local description: \" + JSON.stringify(desc));\n",
       "                    webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.setLocalDescription(desc).then(function() {\n",
       "                        websocketConnection2775ed83b83c42268f496013e8a30f62.send(JSON.stringify({ type: \"sdp\", \"data\": webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.localDescription }));\n",
       "                    }).catch(reportError2775ed83b83c42268f496013e8a30f62);\n",
       "                }\n",
       "\n",
       "                function onIncomingSDP2775ed83b83c42268f496013e8a30f62(sdp) {\n",
       "                    console.log(\"Incoming SDP: \" + JSON.stringify(sdp));\n",
       "                    webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.setRemoteDescription(sdp).catch(reportError2775ed83b83c42268f496013e8a30f62);\n",
       "                    webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.createAnswer().then(onLocalDescription2775ed83b83c42268f496013e8a30f62).catch(reportError2775ed83b83c42268f496013e8a30f62);\n",
       "                }\n",
       "\n",
       "                function onIncomingICE2775ed83b83c42268f496013e8a30f62(ice) {\n",
       "                    let candidate = new RTCIceCandidate(ice);\n",
       "                    console.log(\"Incoming ICE: \" + JSON.stringify(ice));\n",
       "                    webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.addIceCandidate(candidate).catch(reportError2775ed83b83c42268f496013e8a30f62);\n",
       "                }\n",
       "\n",
       "                function onAddRemoteStream2775ed83b83c42268f496013e8a30f62(event) {\n",
       "                    console.log(\"onAddRemoteStream\", event);\n",
       "                    remoteStream2775ed83b83c42268f496013e8a30f62 = event.streams[0];\n",
       "                    for (let i = 0; i < html5VideoElement2775ed83b83c42268f496013e8a30f62.length; ++i) {\n",
       "                        html5VideoElement2775ed83b83c42268f496013e8a30f62[i].srcObject = remoteStream2775ed83b83c42268f496013e8a30f62;\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                function onIceCandidate2775ed83b83c42268f496013e8a30f62(event) {\n",
       "                    if (event.candidate == null){\n",
       "                        console.log(\"ICE candidate null\");\n",
       "                        return;\n",
       "                    }\n",
       "\n",
       "                    console.log(\"Sending ICE candidate out: \" + JSON.stringify(event.candidate));\n",
       "                    websocketConnection2775ed83b83c42268f496013e8a30f62.send(JSON.stringify({ \"type\": \"ice\", \"data\": event.candidate }));\n",
       "                }\n",
       "\n",
       "\n",
       "                function onClose2775ed83b83c42268f496013e8a30f62() {\n",
       "                    websocketConnection2775ed83b83c42268f496013e8a30f62.close();\n",
       "                    console.log(websocketConnection2775ed83b83c42268f496013e8a30f62.readyState)\n",
       "                    websocketConnection2775ed83b83c42268f496013e8a30f62 = null;\n",
       "                    webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.close();\n",
       "                    webrtcPeerConnection2775ed83b83c42268f496013e8a30f62 = null;\n",
       "                    console.log(\"requested to close the stream\");\n",
       "                }\n",
       "\n",
       "                function onServerMessage2775ed83b83c42268f496013e8a30f62(event) {\n",
       "                    let msg;\n",
       "\n",
       "                    try {\n",
       "                        msg = JSON.parse(event.data);\n",
       "                    } catch (e) {\n",
       "                        return;\n",
       "                    }\n",
       "\n",
       "                    if (!webrtcPeerConnection2775ed83b83c42268f496013e8a30f62) {\n",
       "                        webrtcPeerConnection2775ed83b83c42268f496013e8a30f62 = new RTCPeerConnection(webrtcConfiguration2775ed83b83c42268f496013e8a30f62);\n",
       "                        webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.ontrack = onAddRemoteStream2775ed83b83c42268f496013e8a30f62;\n",
       "                        webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.onicecandidate = onIceCandidate2775ed83b83c42268f496013e8a30f62;\n",
       "                        webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.addEventListener('connectionstatechange', event => {\n",
       "                            console.log(\"webrtc state\", webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.connectionState);\n",
       "                        });\n",
       "                        webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.oniceconnectionstatechange = (event) => {\n",
       "                            console.log(\"ice state\", webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.iceConnectionState);\n",
       "                        };\n",
       "                        webrtcPeerConnection2775ed83b83c42268f496013e8a30f62.onicecandidateerror = (event) => {console.log(event)};\n",
       "                    }\n",
       "\n",
       "                    switch (msg.type) {\n",
       "                        case \"sdp\": onIncomingSDP2775ed83b83c42268f496013e8a30f62(msg.data); break;\n",
       "                        case \"ice\": onIncomingICE2775ed83b83c42268f496013e8a30f62(msg.data); break;\n",
       "                        case \"close\": onClose2775ed83b83c42268f496013e8a30f62(); break;\n",
       "                        default: break;\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                function videoClicked2775ed83b83c42268f496013e8a30f62(event) {\n",
       "                    if (websocketConnection2775ed83b83c42268f496013e8a30f62 !== null && websocketConnection2775ed83b83c42268f496013e8a30f62.readyState===1) {\n",
       "                        let x = event.offsetX;\n",
       "                        let y = event.offsetY;\n",
       "                        websocketConnection2775ed83b83c42268f496013e8a30f62.send(JSON.stringify({\n",
       "                            \"type\": \"click\",\n",
       "                            \"data\": {\n",
       "                                'event': 'click',\n",
       "                                'eventData': {\n",
       "                                    'altKey': event.altKey,\n",
       "                                    'ctrlKey': event.ctrlKey,\n",
       "                                    'shiftKey': event.shiftKey,\n",
       "                                    'offsetX': x,\n",
       "                                    'offsetY': y\n",
       "                                }\n",
       "                            }\n",
       "                        }));\n",
       "                    } else {\n",
       "                        playStream2775ed83b83c42268f496013e8a30f62();\n",
       "                    }\n",
       "                    event.preventDefault();\n",
       "                }\n",
       "\n",
       "                function playStream2775ed83b83c42268f496013e8a30f62() {\n",
       "                    if (websocketConnection2775ed83b83c42268f496013e8a30f62 === null) {\n",
       "                        let l = window.location;\n",
       "                        let wsUrl = \"ws://\" + l.hostname + \":1024/webrtc\";\n",
       "\n",
       "                        html5VideoElement2775ed83b83c42268f496013e8a30f62 = document.getElementsByClassName(\"stream_widget_2775ed83b83c42268f496013e8a30f62\");\n",
       "                        for (let i = 0; i < html5VideoElement2775ed83b83c42268f496013e8a30f62.length; ++i) {\n",
       "                            html5VideoElement2775ed83b83c42268f496013e8a30f62[i].removeEventListener(\"mousedown\", videoClicked2775ed83b83c42268f496013e8a30f62, false);\n",
       "                            html5VideoElement2775ed83b83c42268f496013e8a30f62[i].addEventListener(\"mousedown\", videoClicked2775ed83b83c42268f496013e8a30f62, false);\n",
       "                        }\n",
       "                        webrtcConfiguration2775ed83b83c42268f496013e8a30f62 = { 'iceServers': [{urls: \"stun:stun.l.google.com:19302\"}] };\n",
       "                        reportError2775ed83b83c42268f496013e8a30f62 = function (errmsg) { console.error(errmsg); };\n",
       "\n",
       "                        websocketConnection2775ed83b83c42268f496013e8a30f62 = new WebSocket(wsUrl);\n",
       "                        websocketConnection2775ed83b83c42268f496013e8a30f62.addEventListener(\"message\", onServerMessage2775ed83b83c42268f496013e8a30f62);\n",
       "                    } else {\n",
       "                        html5VideoElement2775ed83b83c42268f496013e8a30f62 = document.getElementsByClassName(\"stream_widget_2775ed83b83c42268f496013e8a30f62\");\n",
       "                        for (let i = 0; i < html5VideoElement2775ed83b83c42268f496013e8a30f62.length; ++i) {\n",
       "                            html5VideoElement2775ed83b83c42268f496013e8a30f62[i].srcObject = remoteStream2775ed83b83c42268f496013e8a30f62;\n",
       "                            html5VideoElement2775ed83b83c42268f496013e8a30f62[i].removeEventListener(\"mousedown\", videoClicked2775ed83b83c42268f496013e8a30f62, false);\n",
       "                            html5VideoElement2775ed83b83c42268f496013e8a30f62[i].addEventListener(\"mousedown\", videoClicked2775ed83b83c42268f496013e8a30f62, false);\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "                playStream2775ed83b83c42268f496013e8a30f62();\n",
       "        \n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "_repr_pretty_() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/lib/pretty.py:407\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    405\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pretty_\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(meth):\n\u001b[0;32m--> 407\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _repr_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n",
      "\u001b[0;31mTypeError\u001b[0m: _repr_pretty_() takes 1 positional argument but 3 were given"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '_repr_html_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/clickable_stream_widget-1.0.0-py3.8.egg/clickable_stream_widget/widget.py:326\u001b[0m, in \u001b[0;36mClickableImageWidget._repr_html_\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repr_html_\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '_repr_html_'"
     ]
    }
   ],
   "source": [
    "display(camera_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collect all the data, we can create a dataset.zip file and upload to cloudai for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: apex/ (stored 0%)\n",
      "  adding: apex/4_5_01154bb6-21c1-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/41_69_f1e385c2-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/101_113_f037a352-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: apex/.ipynb_checkpoints/4_5_01154bb6-21c1-11ef-90b9-48ad9a4bd557-checkpoint.jpg (deflated 1%)\n",
      "  adding: apex/92_116_fe91c0d6-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/12_124_f170a9c6-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/136_108_fdf9765a-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/137_71_fe2c697a-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/111_116_feb3337e-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/92_90_fe66ea32-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/221_222_ffcd627a-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/118_116_fed1e102-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/132_122_ff0c6ade-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/155_91_f098cdbc-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/57_73_f0ef6c80-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/61_110_fda55f84-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n",
      "  adding: apex/124_116_fef0291e-21c0-11ef-90b9-48ad9a4bd557.jpg (deflated 1%)\n"
     ]
    }
   ],
   "source": [
    "!cd road_following_A && zip -r - apex > ../dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
